{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11284539,"sourceType":"datasetVersion","datasetId":7055459},{"sourceId":11417315,"sourceType":"datasetVersion","datasetId":6724904},{"sourceId":344169,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":287780,"modelId":308569}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡å¤æ€§\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# æ•°æ®ç›®å½•\nTRAIN_DIR = \"/kaggle/input/1111111/data/train\"\nTEST_DIR = \"/kaggle/input/1111111/data/test\"\n\n# å‚æ•°é…ç½®\nBATCH_SIZE = 128  # å¢žå¤§batch sizeä»¥åˆ©ç”¨GPUå¹¶è¡Œè®¡ç®—\nEPOCHS = 50  # å¢žåŠ epochs\nPATIENCE = 10  # è°ƒæ•´æ—©åœè€å¿ƒå€¼\nLEARNING_RATE = 0.001\nINPUT_SHAPE = (40, 7)  # æ—¶é—´æ­¥é•¿40ï¼Œç‰¹å¾æ•°7\n\ndef load_data_from_dir(directory):\n    \"\"\"\n    åŠ è½½CSVæ ¼å¼çš„æ—¶åºæ•°æ®\n    ç›®å½•ç»“æž„:\n    directory/\n        legit/\n            file1.csv\n            file2.csv\n            ...\n        rise/\n            file1.csv\n            file2.csv\n            ...\n    æ¯ä¸ªCSVæ–‡ä»¶åº”ä¸º (40, 7) çš„å½¢çŠ¶\n    \"\"\"\n    X = []\n    y = []\n    \n    class_mapping = {'legit': 0, 'rise': 1}\n    \n    for class_name in ['legit', 'rise']:\n        class_dir = os.path.join(directory, class_name)\n        if not os.path.exists(class_dir):\n            print(f\"Warning: Directory {class_dir} not found\")\n            continue\n            \n        class_idx = class_mapping[class_name]\n        \n        for filename in sorted(os.listdir(class_dir)):\n            if not filename.endswith('.csv'):\n                continue\n                \n            filepath = os.path.join(class_dir, filename)\n            \n            try:\n                data = pd.read_csv(filepath, header=None).values\n                \n                if data.shape != INPUT_SHAPE:\n                    print(f\"Warning: File {filepath} has shape {data.shape}, expected {INPUT_SHAPE}\")\n                    continue\n                    \n                X.append(data)\n                y.append(class_idx)\n                \n            except Exception as e:\n                print(f\"Error loading {filepath}: {str(e)}\")\n                continue\n    \n    X = np.array(X, dtype=np.float32)\n    y = np.array(y, dtype=np.int32)\n    \n    return X, y\n\n# åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®\nprint(\"Loading training data...\")\nX_train, y_train = load_data_from_dir(TRAIN_DIR)\nprint(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n\nprint(\"\\nLoading test data...\")\nX_test, y_test = load_data_from_dir(TEST_DIR)\nprint(f\"Test data shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n\n# æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§\nprint(\"\\nClass distribution:\")\nprint(f\"Train - Legit: {sum(y_train == 0)}, Rise: {sum(y_train == 1)}\")\nprint(f\"Test - Legit: {sum(y_test == 0)}, Rise: {sum(y_test == 1)}\")\n\n# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\nprint(\"\\nSplitting training set into train/validation...\")\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, \n    test_size=0.2, \n    random_state=42, \n    stratify=y_train\n)\nprint(f\"Final shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n\n\n\n# æž„å»ºä¿®æ­£åŽçš„CNNæ¨¡åž‹\ndef build_optimized_cnn(input_shape):\n    input_layer = layers.Input(shape=input_shape)\n    \n    # ç¬¬ä¸€å·ç§¯å—\n    x = layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(input_layer)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # ç¬¬äºŒå·ç§¯å—\n    x = layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.4)(x)\n    \n    # ç¬¬ä¸‰å·ç§¯å—\n    x = layers.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.5)(x)\n    \n    # ä¿®æ­£åŽçš„æ³¨æ„åŠ›æœºåˆ¶\n    # èŽ·å–ç‰¹å¾å›¾çš„å½¢çŠ¶ (batch_size, timesteps, features)\n    _, timesteps, features = x.shape\n    \n    # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n    attention = layers.GlobalAveragePooling1D()(x)  # (batch_size, features)\n    attention = layers.Dense(features, activation='sigmoid')(attention)  # (batch_size, features)\n    attention = layers.Reshape((1, features))(attention)  # (batch_size, 1, features)\n    \n    # åº”ç”¨æ³¨æ„åŠ›æƒé‡\n    x = layers.Multiply()([x, attention])  # (batch_size, timesteps, features)\n    \n    # å…¨å±€å¹³å‡æ± åŒ–å’Œå…¨è¿žæŽ¥å±‚\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    x = layers.Dense(64, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    output_layer = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = models.Model(inputs=input_layer, outputs=output_layer)\n    \n    return model\n\n# æž„å»ºæ¨¡åž‹\nmodel = build_optimized_cnn(INPUT_SHAPE)\n\n# æ‰“å°æ¨¡åž‹ç»“æž„\nmodel.summary()\n\n# ç¼–è¯‘æ¨¡åž‹\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=LEARNING_RATE,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07\n)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc'),\n        tf.keras.metrics.AUC(name='pr_auc', curve='PR')\n    ]\n)\n\n# å®šä¹‰å›žè°ƒå‡½æ•°\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_pr_auc',\n    patience=PATIENCE,\n    restore_best_weights=True,\n    mode='max'\n)\n\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor='val_pr_auc',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-6,\n    mode='max'\n)\n\nmodel_checkpoint = callbacks.ModelCheckpoint(\n    'best_model.keras',\n    monitor='val_pr_auc',\n    save_best_only=True,\n    mode='max'\n)\n\n# è®­ç»ƒæ¨¡åž‹\nhistory = model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n    verbose=1\n)\n\n# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 8))\n    \n    # å‡†ç¡®çŽ‡\n    plt.subplot(2, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # æŸå¤±\n    plt.subplot(2, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # AUC\n    plt.subplot(2, 2, 3)\n    plt.plot(history.history['auc'], label='Train AUC')\n    plt.plot(history.history['val_auc'], label='Validation AUC')\n    plt.title('ROC AUC')\n    plt.xlabel('Epoch')\n    plt.ylabel('AUC')\n    plt.legend()\n    \n    # PR AUC\n    plt.subplot(2, 2, 4)\n    plt.plot(history.history['pr_auc'], label='Train PR AUC')\n    plt.plot(history.history['val_pr_auc'], label='Validation PR AUC')\n    plt.title('PR AUC')\n    plt.xlabel('Epoch')\n    plt.ylabel('PR AUC')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_metrics.png')\n    plt.show()\n\nplot_training_history(history)\n\n# è¯„ä¼°æ¨¡åž‹\nprint(\"\\nEvaluating on test set...\")\ntest_metrics = model.evaluate(X_test, y_test, verbose=0)\nmetrics_names = model.metrics_names\n\nfor name, value in zip(metrics_names, test_metrics):\n    print(f\"Test {name}: {value:.4f}\")\n\n# ç”Ÿæˆé¢„æµ‹ç»“æžœ\ny_pred = model.predict(X_test)\ny_pred_classes = (y_pred > 0.5).astype(int)\n\n# æ‰“å°åˆ†ç±»æŠ¥å‘Š\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_classes))\n\n# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\ncm = confusion_matrix(y_test, y_pred_classes)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')\nplt.show()\n\n# ä¿å­˜æ¨¡åž‹\nmodel.save('optimized_time_series_cnn_model.keras')\nprint(\"Model saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:47:29.255535Z","iopub.execute_input":"2025-04-16T06:47:29.255854Z","iopub.status.idle":"2025-04-16T06:48:13.998132Z","shell.execute_reply.started":"2025-04-16T06:47:29.255822Z","shell.execute_reply":"2025-04-16T06:48:13.997444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import load_model\n\ndef load_trained_model(model_path='/kaggle/working/best_model.keras'):\n    \"\"\"åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹\"\"\"\n    model = load_model(model_path)\n    return model\n\ndef load_and_preprocess_data_from_directory(directory, expected_shape=(40, 7)):\n    \"\"\"ä»Žç›®å½•åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®\"\"\"\n    data = []\n    file_list = sorted([f for f in os.listdir(directory) \n                       if f.endswith('.csv') and not f.startswith('.')])\n\n    if not file_list:\n        print(f\"è­¦å‘Š: ç›®å½• {directory} ä¸­æ²¡æœ‰CSVæ–‡ä»¶ï¼\")\n        return np.array([])\n\n    for file in file_list:\n        file_path = os.path.join(directory, file)\n        try:\n            df = pd.read_csv(file_path, header=None)\n            if df.shape != expected_shape:\n                print(f\"è·³è¿‡ {file}: å½¢çŠ¶ä¸ç¬¦åˆè¦æ±‚\")\n                continue\n            data.append(df.values.astype('float32'))\n        except Exception as e:\n            print(f\"è¯»å– {file} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\")\n\n    return np.array(data)\n\ndef classify_and_print_results(model, data):\n    \"\"\"æ‰§è¡Œåˆ†ç±»å¹¶æ˜¾ç¤ºå¸¦æ¦‚çŽ‡çš„ç»“æžœ\"\"\"\n    if data.ndim != 3:\n        print(\"é”™è¯¯ï¼šæ•°æ®ç»´åº¦ä¸æ­£ç¡®\")\n        return\n\n    # èŽ·å–åŽŸå§‹æ¦‚çŽ‡é¢„æµ‹ç»“æžœ\n    predictions = model.predict(data)\n    predicted_labels = (predictions > 0.5).astype(int)\n\n    # åˆå§‹åŒ–è®¡æ•°å™¨\n    rise_count, legit_count = 0, 0\n\n    print(\"\\nè¯¦ç»†é¢„æµ‹ç»“æžœ:\")\n    for idx, prob in enumerate(predictions.flatten()):\n        # è§£æžé¢„æµ‹ç»“æžœ\n        label = 'rise' if prob > 0.5 else 'legit'\n        confidence = prob if label == 'rise' else 1 - prob\n        \n        # æ›´æ–°è®¡æ•°å™¨\n        if label == 'rise':\n            rise_count += 1\n        else:\n            legit_count += 1\n            \n        # æ˜¾ç¤ºå¸¦æ¦‚çŽ‡çš„ç»“æžœ\n        print(f\"æ–‡ä»¶ {idx+1}:\")\n        print(f\"  â–ª ç±»åˆ«é¢„æµ‹: {label.upper()}\")\n        print(f\"  â–ª Riseæ¦‚çŽ‡: {prob:.4f}\" if predictions.ndim == 2 else f\"  â–ª Riseæ¦‚çŽ‡: {prob:.4f}\")  # æ ¹æ®ç»´åº¦è°ƒæ•´è®¿é—®æ–¹å¼\n        print(f\"  â–ª ç½®ä¿¡ç¨‹åº¦: {confidence:.2%}\")\n        print(\"-\" * 40)\n\n    # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦\n    print(\"\\né¢„æµ‹ç»Ÿè®¡æ‘˜è¦:\")\n    print(f\"Rise ç±»åˆ«æ•°é‡: {rise_count} ({rise_count/len(predictions):.1%})\")\n    print(f\"Legit ç±»åˆ«æ•°é‡: {legit_count} ({legit_count/len(predictions):.1%})\")\n    print(f\"æ€»æ ·æœ¬æ•°é‡: {len(predictions)}\")\n\ndef main():\n    # é…ç½®è·¯å¾„\n    DATA_DIR = '/kaggle/input/fdp-temp/fdp'\n    MODEL_PATH = '/kaggle/working/best_model.keras'\n\n    # åŠ è½½æ¨¡åž‹\n    print(\"ðŸ”„ æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹...\")\n    try:\n        model = load_trained_model(MODEL_PATH)\n        print(\"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ\")\n    except Exception as e:\n        print(f\"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}\")\n        return\n\n    # åŠ è½½æ•°æ®\n    print(f\"\\nðŸ“‚ æ­£åœ¨ä»Ž {DATA_DIR} åŠ è½½æ•°æ®...\")\n    data = load_and_preprocess_data_from_directory(DATA_DIR)\n    if len(data) == 0:\n        print(\"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®\")\n        return\n    print(f\"âœ”ï¸ æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬\")\n\n    # æ‰§è¡Œé¢„æµ‹\n    print(\"\\nðŸ”® å¼€å§‹æ‰§è¡Œé¢„æµ‹...\")\n    classify_and_print_results(model, data)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:48:13.999156Z","iopub.execute_input":"2025-04-16T06:48:13.999487Z","iopub.status.idle":"2025-04-16T06:48:16.783157Z","shell.execute_reply.started":"2025-04-16T06:48:13.999462Z","shell.execute_reply":"2025-04-16T06:48:16.782487Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#è¯„ä¼°v2\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\ndef load_data_from_directory(directory, label, expected_shape=(40, 7)):\n    \"\"\"\n    ä»ŽæŒ‡å®šç›®å½•åŠ è½½CSVæ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨è¿‡æ»¤ä¸ç¬¦åˆè¦æ±‚çš„æ•°æ®\n    å‚æ•°ï¼š\n        directory: æ•°æ®ç›®å½•è·¯å¾„\n        label: æ•°æ®æ ‡ç­¾\n        expected_shape: é¢„æœŸçš„æ•°æ®å½¢çŠ¶ï¼ˆè¡Œï¼Œåˆ—ï¼‰\n    è¿”å›žï¼š\n        data_array: ä¸‰ç»´numpyæ•°ç»„ (æ ·æœ¬æ•°, è¡Œ, åˆ—)\n        labels_array: ä¸€ç»´numpyæ•°ç»„\n    \"\"\"\n    data = []\n    labels = []\n    valid_count = 0\n    skip_count = 0\n    \n    # èŽ·å–ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶ï¼ˆè‡ªåŠ¨è¿‡æ»¤éšè—æ–‡ä»¶ï¼‰\n    file_list = sorted([f for f in os.listdir(directory) \n                      if f.endswith('.csv') and not f.startswith('.')])\n    \n    if not file_list:\n        print(f\"è­¦å‘Š: ç›®å½• {directory} ä¸­æ²¡æœ‰CSVæ–‡ä»¶ï¼\")\n        return np.array([]), np.array([])\n    \n    for file in file_list:\n        file_path = os.path.join(directory, file)\n        try:\n            df = pd.read_csv(file_path, header=None)\n            \n            # å½¢çŠ¶æ£€æŸ¥\n            if df.shape != expected_shape:\n                print(f\"è·³è¿‡ {file}: æœŸæœ›å½¢çŠ¶ {expected_shape}ï¼Œå®žé™…å½¢çŠ¶ {df.shape}\")\n                skip_count += 1\n                continue\n                \n            data.append(df.values.astype('float32'))  # è½¬æ¢ä¸ºnumpyæ•°ç»„\n            labels.append(label)\n            valid_count += 1\n            \n        except Exception as e:\n            print(f\"è¯»å– {file} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\")\n            skip_count += 1\n    \n    print(f\"ä»Ž {directory} æˆåŠŸåŠ è½½ {valid_count} ä¸ªæ ·æœ¬ï¼Œè·³è¿‡ {skip_count} ä¸ªæ— æ•ˆæ–‡ä»¶\")\n    return np.array(data), np.array(labels)\n\ndef reshape_data(data):\n    \"\"\"ç¡®ä¿æ•°æ®å½¢çŠ¶ä¸º (æ ·æœ¬æ•°, æ—¶é—´æ­¥é•¿, ç‰¹å¾æ•°)\"\"\"\n    # å¦‚æžœæ•°æ®å·²ç»æ˜¯3Då½¢çŠ¶åˆ™ç›´æŽ¥è¿”å›ž\n    if data.ndim == 3:\n        return data\n    # å¦åˆ™è‡ªåŠ¨é‡å¡‘ä¸º3Dï¼ˆä¾‹å¦‚ï¼šå‡è®¾åŽŸå§‹å½¢çŠ¶æ˜¯ (æ ·æœ¬æ•°, 40*7)ï¼‰\n    return data.reshape(-1, 40, 7)  # ä½¿ç”¨å®žé™…çš„æ—¶é—´æ­¥é•¿å’Œç‰¹å¾æ•°\n\n# åŠ è½½æ¨¡åž‹\nmodel = load_model('optimized_time_series_cnn_model.keras')\n\n# é…ç½®å‚æ•°\nTEST_DIR = '/kaggle/input/1111111/data/test'  # æ›¿æ¢ä¸ºä½ çš„æµ‹è¯•é›†è·¯å¾„\nEXPECTED_SHAPE = (40, 7)  # æ ¹æ®å®žé™…æ•°æ®å½¢çŠ¶ä¿®æ”¹\n\n# åŠ è½½æµ‹è¯•æ•°æ®\nprint(\"æ­£åœ¨åŠ è½½legitæµ‹è¯•æ•°æ®...\")\nlegit_data, legit_labels = load_data_from_directory(\n    os.path.join(TEST_DIR, 'legit'), \n    label=0,\n    expected_shape=EXPECTED_SHAPE\n)\n\nprint(\"\\næ­£åœ¨åŠ è½½riseæµ‹è¯•æ•°æ®...\")\nrise_data, rise_labels = load_data_from_directory(\n    os.path.join(TEST_DIR, 'fdp'), \n    label=1,\n    expected_shape=EXPECTED_SHAPE\n)\n\n# æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½æˆåŠŸ\nif len(legit_data) == 0:\n    raise ValueError(\"æ²¡æœ‰åŠ è½½åˆ°legitæµ‹è¯•æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼ï¼\")\nif len(rise_data) == 0:\n    raise ValueError(\"æ²¡æœ‰åŠ è½½åˆ°riseæµ‹è¯•æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼ï¼\")\n\n# åˆå¹¶æ•°æ®å¹¶ä¿æŒåŽŸå§‹é¡ºåº\nX_test = np.vstack((legit_data, rise_data))\ny_test = np.hstack((legit_labels, rise_labels))\n\n# ä¿®æ­£æ•°æ®å½¢çŠ¶\nX_test = reshape_data(X_test)\n\n# è¿›è¡Œé¢„æµ‹\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# è®¡ç®—è¯„ä¼°æŒ‡æ ‡\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# è®¡ç®—å‡†ç¡®çŽ‡\naccuracy = np.mean(y_pred.flatten() == y_test)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\ncm = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(6, 6))\ncax = ax.matshow(cm, cmap='Blues')\nfig.colorbar(cax)\nax.set_xticks([0, 1])\nax.set_yticks([0, 1])\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:48:44.315059Z","iopub.execute_input":"2025-04-16T06:48:44.315352Z","iopub.status.idle":"2025-04-16T06:48:46.025455Z","shell.execute_reply.started":"2025-04-16T06:48:44.315329Z","shell.execute_reply":"2025-04-16T06:48:46.024191Z"}},"outputs":[],"execution_count":null}]}