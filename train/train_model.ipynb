{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11284539,"sourceType":"datasetVersion","datasetId":7055459},{"sourceId":11417315,"sourceType":"datasetVersion","datasetId":6724904},{"sourceId":344169,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":287780,"modelId":308569}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# 设置随机种子保证可重复性\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# 数据目录\nTRAIN_DIR = \"/kaggle/input/1111111/data/train\"\nTEST_DIR = \"/kaggle/input/1111111/data/test\"\n\n# 参数配置\nBATCH_SIZE = 128  # 增大batch size以利用GPU并行计算\nEPOCHS = 50  # 增加epochs\nPATIENCE = 10  # 调整早停耐心值\nLEARNING_RATE = 0.001\nINPUT_SHAPE = (40, 7)  # 时间步长40，特征数7\n\ndef load_data_from_dir(directory):\n    \"\"\"\n    加载CSV格式的时序数据\n    目录结构:\n    directory/\n        legit/\n            file1.csv\n            file2.csv\n            ...\n        rise/\n            file1.csv\n            file2.csv\n            ...\n    每个CSV文件应为 (40, 7) 的形状\n    \"\"\"\n    X = []\n    y = []\n    \n    class_mapping = {'legit': 0, 'rise': 1}\n    \n    for class_name in ['legit', 'rise']:\n        class_dir = os.path.join(directory, class_name)\n        if not os.path.exists(class_dir):\n            print(f\"Warning: Directory {class_dir} not found\")\n            continue\n            \n        class_idx = class_mapping[class_name]\n        \n        for filename in sorted(os.listdir(class_dir)):\n            if not filename.endswith('.csv'):\n                continue\n                \n            filepath = os.path.join(class_dir, filename)\n            \n            try:\n                data = pd.read_csv(filepath, header=None).values\n                \n                if data.shape != INPUT_SHAPE:\n                    print(f\"Warning: File {filepath} has shape {data.shape}, expected {INPUT_SHAPE}\")\n                    continue\n                    \n                X.append(data)\n                y.append(class_idx)\n                \n            except Exception as e:\n                print(f\"Error loading {filepath}: {str(e)}\")\n                continue\n    \n    X = np.array(X, dtype=np.float32)\n    y = np.array(y, dtype=np.int32)\n    \n    return X, y\n\n# 加载训练和测试数据\nprint(\"Loading training data...\")\nX_train, y_train = load_data_from_dir(TRAIN_DIR)\nprint(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n\nprint(\"\\nLoading test data...\")\nX_test, y_test = load_data_from_dir(TEST_DIR)\nprint(f\"Test data shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n\n# 检查数据平衡性\nprint(\"\\nClass distribution:\")\nprint(f\"Train - Legit: {sum(y_train == 0)}, Rise: {sum(y_train == 1)}\")\nprint(f\"Test - Legit: {sum(y_test == 0)}, Rise: {sum(y_test == 1)}\")\n\n# 划分训练集和验证集\nprint(\"\\nSplitting training set into train/validation...\")\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, \n    test_size=0.2, \n    random_state=42, \n    stratify=y_train\n)\nprint(f\"Final shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n\n\n\n# 构建修正后的CNN模型\ndef build_optimized_cnn(input_shape):\n    input_layer = layers.Input(shape=input_shape)\n    \n    # 第一卷积块\n    x = layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(input_layer)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # 第二卷积块\n    x = layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.4)(x)\n    \n    # 第三卷积块\n    x = layers.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = layers.Dropout(0.5)(x)\n    \n    # 修正后的注意力机制\n    # 获取特征图的形状 (batch_size, timesteps, features)\n    _, timesteps, features = x.shape\n    \n    # 计算注意力权重\n    attention = layers.GlobalAveragePooling1D()(x)  # (batch_size, features)\n    attention = layers.Dense(features, activation='sigmoid')(attention)  # (batch_size, features)\n    attention = layers.Reshape((1, features))(attention)  # (batch_size, 1, features)\n    \n    # 应用注意力权重\n    x = layers.Multiply()([x, attention])  # (batch_size, timesteps, features)\n    \n    # 全局平均池化和全连接层\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    x = layers.Dense(64, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    output_layer = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = models.Model(inputs=input_layer, outputs=output_layer)\n    \n    return model\n\n# 构建模型\nmodel = build_optimized_cnn(INPUT_SHAPE)\n\n# 打印模型结构\nmodel.summary()\n\n# 编译模型\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=LEARNING_RATE,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07\n)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc'),\n        tf.keras.metrics.AUC(name='pr_auc', curve='PR')\n    ]\n)\n\n# 定义回调函数\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_pr_auc',\n    patience=PATIENCE,\n    restore_best_weights=True,\n    mode='max'\n)\n\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor='val_pr_auc',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-6,\n    mode='max'\n)\n\nmodel_checkpoint = callbacks.ModelCheckpoint(\n    'best_model.keras',\n    monitor='val_pr_auc',\n    save_best_only=True,\n    mode='max'\n)\n\n# 训练模型\nhistory = model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n    verbose=1\n)\n\n# 绘制训练曲线\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 8))\n    \n    # 准确率\n    plt.subplot(2, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # 损失\n    plt.subplot(2, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # AUC\n    plt.subplot(2, 2, 3)\n    plt.plot(history.history['auc'], label='Train AUC')\n    plt.plot(history.history['val_auc'], label='Validation AUC')\n    plt.title('ROC AUC')\n    plt.xlabel('Epoch')\n    plt.ylabel('AUC')\n    plt.legend()\n    \n    # PR AUC\n    plt.subplot(2, 2, 4)\n    plt.plot(history.history['pr_auc'], label='Train PR AUC')\n    plt.plot(history.history['val_pr_auc'], label='Validation PR AUC')\n    plt.title('PR AUC')\n    plt.xlabel('Epoch')\n    plt.ylabel('PR AUC')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_metrics.png')\n    plt.show()\n\nplot_training_history(history)\n\n# 评估模型\nprint(\"\\nEvaluating on test set...\")\ntest_metrics = model.evaluate(X_test, y_test, verbose=0)\nmetrics_names = model.metrics_names\n\nfor name, value in zip(metrics_names, test_metrics):\n    print(f\"Test {name}: {value:.4f}\")\n\n# 生成预测结果\ny_pred = model.predict(X_test)\ny_pred_classes = (y_pred > 0.5).astype(int)\n\n# 打印分类报告\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_classes))\n\n# 绘制混淆矩阵\ncm = confusion_matrix(y_test, y_pred_classes)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')\nplt.show()\n\n# 保存模型\nmodel.save('optimized_time_series_cnn_model.keras')\nprint(\"Model saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:47:29.255535Z","iopub.execute_input":"2025-04-16T06:47:29.255854Z","iopub.status.idle":"2025-04-16T06:48:13.998132Z","shell.execute_reply.started":"2025-04-16T06:47:29.255822Z","shell.execute_reply":"2025-04-16T06:48:13.997444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import load_model\n\ndef load_trained_model(model_path='/kaggle/working/best_model.keras'):\n    \"\"\"加载训练好的模型\"\"\"\n    model = load_model(model_path)\n    return model\n\ndef load_and_preprocess_data_from_directory(directory, expected_shape=(40, 7)):\n    \"\"\"从目录加载并预处理数据\"\"\"\n    data = []\n    file_list = sorted([f for f in os.listdir(directory) \n                       if f.endswith('.csv') and not f.startswith('.')])\n\n    if not file_list:\n        print(f\"警告: 目录 {directory} 中没有CSV文件！\")\n        return np.array([])\n\n    for file in file_list:\n        file_path = os.path.join(directory, file)\n        try:\n            df = pd.read_csv(file_path, header=None)\n            if df.shape != expected_shape:\n                print(f\"跳过 {file}: 形状不符合要求\")\n                continue\n            data.append(df.values.astype('float32'))\n        except Exception as e:\n            print(f\"读取 {file} 时发生错误: {str(e)}\")\n\n    return np.array(data)\n\ndef classify_and_print_results(model, data):\n    \"\"\"执行分类并显示带概率的结果\"\"\"\n    if data.ndim != 3:\n        print(\"错误：数据维度不正确\")\n        return\n\n    # 获取原始概率预测结果\n    predictions = model.predict(data)\n    predicted_labels = (predictions > 0.5).astype(int)\n\n    # 初始化计数器\n    rise_count, legit_count = 0, 0\n\n    print(\"\\n详细预测结果:\")\n    for idx, prob in enumerate(predictions.flatten()):\n        # 解析预测结果\n        label = 'rise' if prob > 0.5 else 'legit'\n        confidence = prob if label == 'rise' else 1 - prob\n        \n        # 更新计数器\n        if label == 'rise':\n            rise_count += 1\n        else:\n            legit_count += 1\n            \n        # 显示带概率的结果\n        print(f\"文件 {idx+1}:\")\n        print(f\"  ▪ 类别预测: {label.upper()}\")\n        print(f\"  ▪ Rise概率: {prob:.4f}\" if predictions.ndim == 2 else f\"  ▪ Rise概率: {prob:.4f}\")  # 根据维度调整访问方式\n        print(f\"  ▪ 置信程度: {confidence:.2%}\")\n        print(\"-\" * 40)\n\n    # 显示统计摘要\n    print(\"\\n预测统计摘要:\")\n    print(f\"Rise 类别数量: {rise_count} ({rise_count/len(predictions):.1%})\")\n    print(f\"Legit 类别数量: {legit_count} ({legit_count/len(predictions):.1%})\")\n    print(f\"总样本数量: {len(predictions)}\")\n\ndef main():\n    # 配置路径\n    DATA_DIR = '/kaggle/input/fdp-temp/fdp'\n    MODEL_PATH = '/kaggle/working/best_model.keras'\n\n    # 加载模型\n    print(\"🔄 正在加载训练好的模型...\")\n    try:\n        model = load_trained_model(MODEL_PATH)\n        print(\"✅ 模型加载成功\")\n    except Exception as e:\n        print(f\"❌ 模型加载失败: {str(e)}\")\n        return\n\n    # 加载数据\n    print(f\"\\n📂 正在从 {DATA_DIR} 加载数据...\")\n    data = load_and_preprocess_data_from_directory(DATA_DIR)\n    if len(data) == 0:\n        print(\"⚠️ 未找到有效数据\")\n        return\n    print(f\"✔️ 成功加载 {len(data)} 个样本\")\n\n    # 执行预测\n    print(\"\\n🔮 开始执行预测...\")\n    classify_and_print_results(model, data)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:48:13.999156Z","iopub.execute_input":"2025-04-16T06:48:13.999487Z","iopub.status.idle":"2025-04-16T06:48:16.783157Z","shell.execute_reply.started":"2025-04-16T06:48:13.999462Z","shell.execute_reply":"2025-04-16T06:48:16.782487Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#评估v2\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\ndef load_data_from_directory(directory, label, expected_shape=(40, 7)):\n    \"\"\"\n    从指定目录加载CSV文件，并自动过滤不符合要求的数据\n    参数：\n        directory: 数据目录路径\n        label: 数据标签\n        expected_shape: 预期的数据形状（行，列）\n    返回：\n        data_array: 三维numpy数组 (样本数, 行, 列)\n        labels_array: 一维numpy数组\n    \"\"\"\n    data = []\n    labels = []\n    valid_count = 0\n    skip_count = 0\n    \n    # 获取目录下所有CSV文件（自动过滤隐藏文件）\n    file_list = sorted([f for f in os.listdir(directory) \n                      if f.endswith('.csv') and not f.startswith('.')])\n    \n    if not file_list:\n        print(f\"警告: 目录 {directory} 中没有CSV文件！\")\n        return np.array([]), np.array([])\n    \n    for file in file_list:\n        file_path = os.path.join(directory, file)\n        try:\n            df = pd.read_csv(file_path, header=None)\n            \n            # 形状检查\n            if df.shape != expected_shape:\n                print(f\"跳过 {file}: 期望形状 {expected_shape}，实际形状 {df.shape}\")\n                skip_count += 1\n                continue\n                \n            data.append(df.values.astype('float32'))  # 转换为numpy数组\n            labels.append(label)\n            valid_count += 1\n            \n        except Exception as e:\n            print(f\"读取 {file} 时发生错误: {str(e)}\")\n            skip_count += 1\n    \n    print(f\"从 {directory} 成功加载 {valid_count} 个样本，跳过 {skip_count} 个无效文件\")\n    return np.array(data), np.array(labels)\n\ndef reshape_data(data):\n    \"\"\"确保数据形状为 (样本数, 时间步长, 特征数)\"\"\"\n    # 如果数据已经是3D形状则直接返回\n    if data.ndim == 3:\n        return data\n    # 否则自动重塑为3D（例如：假设原始形状是 (样本数, 40*7)）\n    return data.reshape(-1, 40, 7)  # 使用实际的时间步长和特征数\n\n# 加载模型\nmodel = load_model('optimized_time_series_cnn_model.keras')\n\n# 配置参数\nTEST_DIR = '/kaggle/input/1111111/data/test'  # 替换为你的测试集路径\nEXPECTED_SHAPE = (40, 7)  # 根据实际数据形状修改\n\n# 加载测试数据\nprint(\"正在加载legit测试数据...\")\nlegit_data, legit_labels = load_data_from_directory(\n    os.path.join(TEST_DIR, 'legit'), \n    label=0,\n    expected_shape=EXPECTED_SHAPE\n)\n\nprint(\"\\n正在加载rise测试数据...\")\nrise_data, rise_labels = load_data_from_directory(\n    os.path.join(TEST_DIR, 'fdp'), \n    label=1,\n    expected_shape=EXPECTED_SHAPE\n)\n\n# 检查数据是否加载成功\nif len(legit_data) == 0:\n    raise ValueError(\"没有加载到legit测试数据，请检查路径和文件格式！\")\nif len(rise_data) == 0:\n    raise ValueError(\"没有加载到rise测试数据，请检查路径和文件格式！\")\n\n# 合并数据并保持原始顺序\nX_test = np.vstack((legit_data, rise_data))\ny_test = np.hstack((legit_labels, rise_labels))\n\n# 修正数据形状\nX_test = reshape_data(X_test)\n\n# 进行预测\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# 计算评估指标\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# 计算准确率\naccuracy = np.mean(y_pred.flatten() == y_test)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# 绘制混淆矩阵\ncm = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(6, 6))\ncax = ax.matshow(cm, cmap='Blues')\nfig.colorbar(cax)\nax.set_xticks([0, 1])\nax.set_yticks([0, 1])\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:48:44.315059Z","iopub.execute_input":"2025-04-16T06:48:44.315352Z","iopub.status.idle":"2025-04-16T06:48:46.025455Z","shell.execute_reply.started":"2025-04-16T06:48:44.315329Z","shell.execute_reply":"2025-04-16T06:48:46.024191Z"}},"outputs":[],"execution_count":null}]}